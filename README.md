# ğŸ’³ Transactly â€” Privacy-First Explainable AI for Smart Transaction Intelligence

> **Built for GHCI 2025 Hackathon â€” Theme: Automated AI Transaction Categorisation**

Transactly is a **fully offline, explainable AI engine** that automatically categorises financial transactions like
`"AMZN Pmt #4827" â†’ "Shopping"`
using a hybrid of **rule-based precision** and **lightweight machine learning**.

It is designed to be **privacy-first**, **interpretable**, and **deployable offline**, combining text-normalisation, embeddings, logistic regression, and explainability.

-----

## ğŸŒŸ Key Features

  * ğŸ§  **Offline AI Engine** â€” No external APIs, runs fully local.
  * ğŸª¶ **Lightweight Embeddings** â€” Uses `all-MiniLM-L6-v2` (\~90 MB).
  * âš™ï¸ **Hybrid Categorisation** â€” Rules + ML + Confidence logic.
  * ğŸ” **Explainability Layer** â€” Shows top-3 similar merchants.
  * ğŸ” **Feedback-Driven Learning** â€” User corrections retrain model.
  * ğŸ§© **Modular Architecture** â€” Python + FastAPI + Streamlit.
  * ğŸ³ **Docker-ready** â€” Single-command portable demo.

-----

## ğŸ—ï¸ Project Architecture

```text
transactly-ai/
â”‚
â”œâ”€â”€ app/                # FastAPI backend
â”‚   â”œâ”€â”€ main.py           # API entrypoint
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ classify.py   # /api/classify endpoint
â”‚   â”‚   â””â”€â”€ feedback.py   # /api/feedback endpoint
â”‚   â””â”€â”€ core/
â”‚       â”œâ”€â”€ category_taxonomy.py
â”‚       â”œâ”€â”€ preprocessing.py
â”‚       â”œâ”€â”€ embeddings.py
â”‚       â”œâ”€â”€ classifier.py
â”‚       â”œâ”€â”€ rules.py
â”‚       â””â”€â”€ decision.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py   # Synthetic data generator
â”‚   â””â”€â”€ retrain.py        # Active learning retrain loop
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/        # CSVs, embeddings, etc.
â”‚   â””â”€â”€ feedback.csv      # User corrections
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py  # Streamlit demo dashboard
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

-----

## âš™ï¸ Tech Stack

| Layer | Technology |
|---|---|
| **Language** | Python 3.11 |
| **ML** | scikit-learn (LogisticRegression), Sentence-Transformers |
| **API** | FastAPI |
| **Frontend** | Streamlit |
| **Storage** | CSV (local) |
| **Explainability** | Cosine similarity of embeddings |
| **Deployment** | Render (backend) + Streamlit Cloud (frontend) |

-----

## ğŸš€ Getting Started (Local Development)

### 1ï¸âƒ£ Clone & setup environment

```bash
git clone https://github.com/manojmg/transactly-ai.git
cd transactly-ai
python -m venv .venv
source .venv/bin/activate   # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

### 2ï¸âƒ£ Prepare data & train model

```bash
python -m scripts.prepare_data
python -m app.core.embeddings
python -m app.core.classifier
```

### 3ï¸âƒ£ Run FastAPI backend

```bash
uvicorn app.main:app --reload
```

â†’ Open [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

### 4ï¸âƒ£ Run Streamlit UI

```bash
streamlit run ui/streamlit_app.py
```

â†’ Open http://localhost:8501

### ğŸ” Feedback & Retraining

Users can submit corrections through the Streamlit interface.
Feedback is logged into `data/feedback.csv`.

To merge feedback and retrain:

```bash
python scripts/retrain.py
```

This regenerates embeddings and updates `app/models/classifier.pkl`.

## ğŸŒ Deployment

### ğŸ§© Backend (FastAPI on Render)

1.  Push repo to GitHub.
2.  Create a new **Render Web Service**.
      * **Build command**: `pip install -r requirements.txt`
      * **Start command**: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`
3.  Wait for deploy â†’ youâ€™ll get a public URL like
    `https://transactly-backend.onrender.com`

### ğŸ’» Frontend (Streamlit Cloud)

1.  Go to [https://share.streamlit.io](https://share.streamlit.io)
2.  Choose this repo.
3.  Set **Main file path**: `ui/streamlit_app.py`
4.  Add an environment variable (TOML syntax):
    `BACKEND_URL = "https://transactly-backend.onrender.com"`
5.  Click **Deploy** â†’ youâ€™ll get
    `https://transactly-ui.streamlit.app`

âœ… Your app now runs end-to-end online and locally.

### ğŸŒ Environment-Aware Configuration

`ui/streamlit_app.py` automatically switches environments:

```python
import os
BACKEND_URL = os.getenv("BACKEND_URL", "http://127.0.0.1:8000")
API_URL = f"{BACKEND_URL}/api/classify/"
FEEDBACK_URL = f"{BACKEND_URL}/api/feedback/"
```

So it â€œjust worksâ€:

  * **Locally** â†’ connects to `localhost:8000`
  * **On Cloud** â†’ connects to Render backend via env var

### ğŸ³ Docker (optional offline bundle)

Build & run both backend + frontend together:

```bash
docker build -t transactly .
docker run -p 8000:8000 -p 8501:8501 transactly
```

Then open:

  * **FastAPI** â†’ http://localhost:8000/docs
  * **Streamlit** â†’ http://localhost:8501

-----

## ğŸ§  Example Inference

```text
Input:   "IRCTC Train Booking #7845"
Output:  Travel & Transport  (rule-based, 100%)

Input:   "AMZN Pmt #8392"
Output:  Shopping  (model-based, 91%)

Input:   "Netflix Subscription"
Output:  Entertainment  (rule-based, 100%)
```

-----

## ğŸ§© Explainability Layer

Each prediction includes:

  * **Method**: `rule` or `model`
  * **Confidence**: probability from classifier
  * **Explanation**: matched rule or similar transactions

Example JSON response:

```json
{
  "description": "Starbucks Order",
  "final_category": "Food & Dining",
  "method": "model",
  "confidence": 0.87,
  "similar_examples": [
    ["Swiggy", 0.93],
    ["Zomato", 0.91],
    ["McDonalds", 0.89]
  ]
}
```

-----

## ğŸ§¾ Notes

  * Render Free Tier may â€œsleepâ€ after 15 min inactivity (cold start delay â‰ˆ 10 s).
  * All AI runs locally â€“ no external API calls or internet inference.
  * Feedback is stored locally (`feedback.csv`), retrainable anytime.

-----

## ğŸ§© Contributors

| Name | Role |
|---|---|
| Manoj MG | Core AI Architecture â€¢ Backend (FastAPI) â€¢ Model & Explainability |
| Mercy | UI & Streamlit Design â€¢ Documentation â€¢ Pitch Slides & Demo Video |
